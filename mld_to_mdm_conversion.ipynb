{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert MLD output to MDM output and visualize it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDM output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion <class 'numpy.ndarray'> (3, 22, 3, 120)\n",
      "text <class 'list'> ['The man spun in place three times.', 'The man spun in place three times.', 'The man spun in place three times.']\n",
      "lengths <class 'numpy.ndarray'> (3,)\n",
      "num_samples <class 'int'> 1\n",
      "num_repetitions <class 'int'> 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mdm_data = np.load('/home/ctq566/motion-diffusion-model/save/humanml_trans_enc_512/samples_humanml_trans_enc_512_000200000_seed10_The_man_spun_in_place_three_times/results.npy', allow_pickle=True).item()\n",
    "# print all keys and value type of mdm_data, if it has shape attribute, print its shape in the same line, otherwise print its value\n",
    "for key, value in mdm_data.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(key, type(value), value.shape)\n",
    "    else:\n",
    "        print(key, type(value), value)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLD output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 22, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mld_data = np.load('/home/ctq566/motion-diffusion-model/playground/Example_50_batch0_0.npy', allow_pickle=True)\n",
    "mld_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mld_data_dict = {'motion': mld_data.transpose((1, 2, 0)).reshape((1, 22, 3, 50)), 'text': 'No text', 'lengths': np.asarray([50]), 'num_samples': 1, 'num_repetitions': 1}\n",
    "np.save('results.npy', mld_data_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The conversion and the stick animation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /home/pjr726/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pjr726/miniconda3/envs/mld/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/pjr726/.local/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/pjr726/.local/lib/python3.9/site-packages (from matplotlib) (5.10.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/pjr726/miniconda3/envs/mld/lib/python3.9/site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/pjr726/miniconda3/envs/mld/lib/python3.9/site-packages (from matplotlib) (9.4.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.3-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/pjr726/.local/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/pjr726/.local/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/pjr726/.local/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/pjr726/miniconda3/envs/mld/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.13.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/pjr726/.local/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: fonttools, matplotlib\n",
      "Successfully installed fonttools-4.39.3 matplotlib-3.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/pjr726/motion-diffusion-model/')\n",
    "import numpy as np\n",
    "import data_loaders.humanml.utils.paramUtil as paramUtil\n",
    "from data_loaders.humanml.utils.plot_script import plot_3d_motion\n",
    "\n",
    "def mld_to_mdm(mld_data_path):\n",
    "    assert isinstance(mld_data_path, str)\n",
    "    mld_data = np.load(mld_data_path, allow_pickle=True)\n",
    "    lengths = mld_data.shape[0]\n",
    "    mld_data_dict = {'motion': mld_data.transpose((1, 2, 0)).reshape((1, 22, 3, lengths)), 'text': 'No text', 'lengths': np.asarray([lengths]), 'num_samples': 1, 'num_repetitions': 1}\n",
    "    # save the result to the same directory as results.npy\n",
    "    np.save(f'{os.path.dirname(mld_data_path)}/results.npy', mld_data_dict)\n",
    "\n",
    "    animation_save_path = f'{os.path.dirname(mld_data_path)}/sample00_rep00.mp4'\n",
    "    skeleton = paramUtil.t2m_kinematic_chain\n",
    "    motion = mld_data\n",
    "    dataset = 'humanml'\n",
    "    title = ''\n",
    "    fps = 20\n",
    "    plot_3d_motion(animation_save_path, skeleton, motion, title, dataset, fps=fps)\n",
    "\n",
    "    \n",
    "# mld_to_mdm('/home/pjr726/motion-latent-diffusion/results/mld/1222_PELearn_Diff_Latent1_MEncDec49_MdiffEnc49_bs64_clip_uncond75_01/random_test/random_196_5.npy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting mld data in /home/pjr726/motion-latent-diffusion/results/mld/now_PELearn_Diff_Latent1_MEncDec49_MdiffEnc49_bs64_clip_uncond75_01/samples_2023-05-11-21-50-11 to mdm data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:30<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def batch_convert_mld_to_mdm(mld_data_dir):\n",
    "    assert isinstance(mld_data_dir, str)\n",
    "    assert os.path.isdir(mld_data_dir)\n",
    "    print(f'Converting mld data in {mld_data_dir} to mdm data')\n",
    "    for mld_data_path in tqdm.tqdm(glob.glob(f'{mld_data_dir}/*.npy')):\n",
    "        # create a new directory for each mld data\n",
    "        new_dir = f'{mld_data_dir}/{os.path.basename(mld_data_path).split(\".\")[0]}'\n",
    "        os.mkdir(new_dir)\n",
    "        # move the mld data to the new directory\n",
    "        os.rename(mld_data_path,\n",
    "                  f'{new_dir}/{os.path.basename(mld_data_path)}')\n",
    "        # change the mld_data_path to the new directory\n",
    "        mld_data_path = f'{new_dir}/{os.path.basename(mld_data_path)}'\n",
    "        # convert the mld data to mdm data\n",
    "        mld_to_mdm(mld_data_path)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "batch_convert_mld_to_mdm(\n",
    "    '/home/pjr726/motion-latent-diffusion/results/mld/now_PELearn_Diff_Latent1_MEncDec49_MdiffEnc49_bs64_clip_uncond75_01/samples_2023-05-11-21-50-11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/pjr726/miniconda3/envs/mdm/lib/python3.7/site-packages (from opencv-python) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.7.0.72\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('/home/pjr726/motion-latent-diffusion/results/mld/now_PELearn_Diff_Latent1_MEncDec49_MdiffEnc49_bs64_clip_uncond75_01/samples_2023-05-11-21-50-11/Example_120_batch0_2_5/sample00_rep00.mp4')\n",
    "# Get the total number of frames in the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Set the number of frames you want to extract\n",
    "num_frames = 12\n",
    "# Calculate the frame interval\n",
    "frame_interval = math.floor(total_frames / num_frames)\n",
    "# Initialize an empty list to store the frames\n",
    "frames = []\n",
    "# Loop through the video frames and extract the frames at the specified interval\n",
    "for i in range(0, total_frames, frame_interval):\n",
    "    # Set the frame index\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "    # Remove the white border (assuming a 5-pixel border)\n",
    "    # frame = cv2.copyMakeBorder(frame, 30, 20, 20, 30, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    h, w, _ = frame.shape\n",
    "    # The crop size is 200x100 for MDM single output only\n",
    "    cropped = frame[(h-150)//2:(h-200)//2+200, (w-100)//2:(w-100)//2+100, :]\n",
    "    # Add the frame to the list\n",
    "    frames.append(cropped)\n",
    "# Concatenate the frames horizontally\n",
    "frame_array = cv2.hconcat(frames)\n",
    "# Save the concatenated frames as a single image with higher resolution\n",
    "cv2.imwrite('motion_boxing.png', frame_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_render",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1111a62abb0ff3902f3bd34c143fce109a1aaad97a527f88625236ee3cab4a67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
